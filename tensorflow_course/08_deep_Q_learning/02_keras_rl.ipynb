{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "Keras-RL2 Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Deep RL Libs:\n",
    "- OpenAI Baselines\n",
    "- Stable Baselines\n",
    "- TF Agents\n",
    "- TensorForce\n",
    "- Keras-RL\n",
    "- Keras-RL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**setup env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def try_out():\n",
    "    env.reset()\n",
    "    for step in range(200):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        env.step(action)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_obs = env.observation_space.shape[0]\n",
    "nb_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1, nb_obs)))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANN Replay Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "memory = SequentialMemory(limit=20000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                                attr='eps', \n",
    "                                value_max=1.0, \n",
    "                                value_min=0.1,\n",
    "                                value_test=0.05,\n",
    "                                nb_steps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQNAgent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=100, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    17/20000: episode: 1, duration: 0.313s, episode steps:  17, steps per second:  54, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.850269, mae: 0.782687, mean_q: 0.402218, mean_eps: 0.999392\n",
      "    37/20000: episode: 2, duration: 0.079s, episode steps:  20, steps per second: 254, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.676901, mae: 0.781404, mean_q: 0.518645, mean_eps: 0.998808\n",
      "    58/20000: episode: 3, duration: 0.080s, episode steps:  21, steps per second: 264, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.360388, mae: 0.674159, mean_q: 0.545165, mean_eps: 0.997885\n",
      "    76/20000: episode: 4, duration: 0.075s, episode steps:  18, steps per second: 239, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.185628, mae: 0.655732, mean_q: 0.763350, mean_eps: 0.997008\n",
      "    91/20000: episode: 5, duration: 0.062s, episode steps:  15, steps per second: 241, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.100850, mae: 0.659047, mean_q: 0.986488, mean_eps: 0.996265\n",
      "   101/20000: episode: 6, duration: 0.041s, episode steps:  10, steps per second: 246, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.092066, mae: 0.715495, mean_q: 1.180923, mean_eps: 0.995702\n",
      "   123/20000: episode: 7, duration: 0.083s, episode steps:  22, steps per second: 267, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.448899, mae: 1.144706, mean_q: 1.517680, mean_eps: 0.994982\n",
      "   152/20000: episode: 8, duration: 0.118s, episode steps:  29, steps per second: 246, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.214338, mae: 1.190593, mean_q: 2.095310, mean_eps: 0.993835\n",
      "   189/20000: episode: 9, duration: 0.141s, episode steps:  37, steps per second: 263, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.145443, mae: 1.144538, mean_q: 2.130015, mean_eps: 0.992350\n",
      "   207/20000: episode: 10, duration: 0.070s, episode steps:  18, steps per second: 258, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.275359, mae: 1.267362, mean_q: 2.194345, mean_eps: 0.991113\n",
      "   217/20000: episode: 11, duration: 0.047s, episode steps:  10, steps per second: 211, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.349426, mae: 1.656025, mean_q: 2.776385, mean_eps: 0.990483\n",
      "   265/20000: episode: 12, duration: 0.181s, episode steps:  48, steps per second: 265, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.284228, mae: 1.725228, mean_q: 3.329860, mean_eps: 0.989177\n",
      "   284/20000: episode: 13, duration: 0.078s, episode steps:  19, steps per second: 244, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.200965, mae: 1.692000, mean_q: 3.254877, mean_eps: 0.987670\n",
      "   315/20000: episode: 14, duration: 0.122s, episode steps:  31, steps per second: 254, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.352702, mae: 1.916532, mean_q: 3.439337, mean_eps: 0.986545\n",
      "   335/20000: episode: 15, duration: 0.075s, episode steps:  20, steps per second: 268, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.342258, mae: 2.271055, mean_q: 4.421312, mean_eps: 0.985398\n",
      "   348/20000: episode: 16, duration: 0.058s, episode steps:  13, steps per second: 225, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.429907, mae: 2.272248, mean_q: 4.269038, mean_eps: 0.984655\n",
      "   363/20000: episode: 17, duration: 0.057s, episode steps:  15, steps per second: 262, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.307522, mae: 2.206549, mean_q: 4.143814, mean_eps: 0.984025\n",
      "   400/20000: episode: 18, duration: 0.138s, episode steps:  37, steps per second: 268, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.282925, mae: 2.218286, mean_q: 4.269784, mean_eps: 0.982855\n",
      "   423/20000: episode: 19, duration: 0.106s, episode steps:  23, steps per second: 217, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 0.592509, mae: 2.738744, mean_q: 4.908756, mean_eps: 0.981505\n",
      "   439/20000: episode: 20, duration: 0.083s, episode steps:  16, steps per second: 193, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.673759, mae: 2.787441, mean_q: 5.309413, mean_eps: 0.980627\n",
      "   453/20000: episode: 21, duration: 0.061s, episode steps:  14, steps per second: 229, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.512317, mae: 2.760610, mean_q: 5.128935, mean_eps: 0.979952\n",
      "   481/20000: episode: 22, duration: 0.117s, episode steps:  28, steps per second: 240, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.455985, mae: 2.775522, mean_q: 5.336127, mean_eps: 0.979008\n",
      "   502/20000: episode: 23, duration: 0.083s, episode steps:  21, steps per second: 254, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.576787, mae: 2.795450, mean_q: 5.253905, mean_eps: 0.977905\n",
      "   525/20000: episode: 24, duration: 0.087s, episode steps:  23, steps per second: 265, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.635564, mae: 3.220054, mean_q: 5.997853, mean_eps: 0.976915\n",
      "   534/20000: episode: 25, duration: 0.035s, episode steps:   9, steps per second: 258, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.612204, mae: 3.274929, mean_q: 6.442610, mean_eps: 0.976195\n",
      "   545/20000: episode: 26, duration: 0.049s, episode steps:  11, steps per second: 225, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.598029, mae: 3.263149, mean_q: 6.316416, mean_eps: 0.975745\n",
      "   563/20000: episode: 27, duration: 0.082s, episode steps:  18, steps per second: 220, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.734547, mae: 3.295479, mean_q: 6.317191, mean_eps: 0.975093\n",
      "   616/20000: episode: 28, duration: 0.221s, episode steps:  53, steps per second: 239, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.745301, mae: 3.375736, mean_q: 6.422030, mean_eps: 0.973495\n",
      "   633/20000: episode: 29, duration: 0.066s, episode steps:  17, steps per second: 259, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.987740, mae: 3.883490, mean_q: 7.592782, mean_eps: 0.971920\n",
      "   651/20000: episode: 30, duration: 0.070s, episode steps:  18, steps per second: 256, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.632048, mae: 3.765296, mean_q: 7.406724, mean_eps: 0.971132\n",
      "   674/20000: episode: 31, duration: 0.090s, episode steps:  23, steps per second: 255, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.553278, mae: 3.763392, mean_q: 7.504873, mean_eps: 0.970210\n",
      "   691/20000: episode: 32, duration: 0.068s, episode steps:  17, steps per second: 250, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.765 [0.000, 1.000],  loss: 0.723517, mae: 3.768465, mean_q: 7.421208, mean_eps: 0.969310\n",
      "   714/20000: episode: 33, duration: 0.099s, episode steps:  23, steps per second: 233, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.304 [0.000, 1.000],  loss: 0.727968, mae: 4.088729, mean_q: 7.905589, mean_eps: 0.968410\n",
      "   733/20000: episode: 34, duration: 0.074s, episode steps:  19, steps per second: 255, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.978988, mae: 4.454563, mean_q: 8.747172, mean_eps: 0.967465\n",
      "   762/20000: episode: 35, duration: 0.111s, episode steps:  29, steps per second: 260, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 0.817315, mae: 4.343880, mean_q: 8.493699, mean_eps: 0.966385\n",
      "   772/20000: episode: 36, duration: 0.040s, episode steps:  10, steps per second: 251, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.108697, mae: 4.398060, mean_q: 8.522468, mean_eps: 0.965508\n",
      "   784/20000: episode: 37, duration: 0.047s, episode steps:  12, steps per second: 253, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.847059, mae: 4.325559, mean_q: 8.388966, mean_eps: 0.965012\n",
      "   799/20000: episode: 38, duration: 0.064s, episode steps:  15, steps per second: 235, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.073311, mae: 4.403135, mean_q: 8.643679, mean_eps: 0.964405\n",
      "   837/20000: episode: 39, duration: 0.143s, episode steps:  38, steps per second: 266, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.605 [0.000, 1.000],  loss: 1.153306, mae: 4.746750, mean_q: 8.992054, mean_eps: 0.963213\n",
      "   854/20000: episode: 40, duration: 0.069s, episode steps:  17, steps per second: 246, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.256350, mae: 4.718264, mean_q: 9.134819, mean_eps: 0.961975\n",
      "   867/20000: episode: 41, duration: 0.055s, episode steps:  13, steps per second: 236, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.131589, mae: 4.762942, mean_q: 9.243727, mean_eps: 0.961300\n",
      "   887/20000: episode: 42, duration: 0.076s, episode steps:  20, steps per second: 264, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.069407, mae: 4.733627, mean_q: 9.174661, mean_eps: 0.960557\n",
      "   906/20000: episode: 43, duration: 0.072s, episode steps:  19, steps per second: 265, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.911962, mae: 4.857234, mean_q: 9.214999, mean_eps: 0.959680\n",
      "   939/20000: episode: 44, duration: 0.130s, episode steps:  33, steps per second: 254, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.199345, mae: 5.165665, mean_q: 9.968190, mean_eps: 0.958510\n",
      "   972/20000: episode: 45, duration: 0.145s, episode steps:  33, steps per second: 228, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.235978, mae: 5.181049, mean_q: 9.976089, mean_eps: 0.957025\n",
      "  1008/20000: episode: 46, duration: 0.162s, episode steps:  36, steps per second: 222, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.169058, mae: 5.206072, mean_q: 9.888688, mean_eps: 0.955472\n",
      "  1024/20000: episode: 47, duration: 0.062s, episode steps:  16, steps per second: 257, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.039096, mae: 5.590777, mean_q: 10.880961, mean_eps: 0.954303\n",
      "  1035/20000: episode: 48, duration: 0.043s, episode steps:  11, steps per second: 259, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.720164, mae: 5.533847, mean_q: 10.447599, mean_eps: 0.953695\n",
      "  1061/20000: episode: 49, duration: 0.099s, episode steps:  26, steps per second: 261, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.278711, mae: 5.519622, mean_q: 10.618378, mean_eps: 0.952863\n",
      "  1111/20000: episode: 50, duration: 0.192s, episode steps:  50, steps per second: 260, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 1.095214, mae: 5.598161, mean_q: 10.827452, mean_eps: 0.951152\n",
      "  1138/20000: episode: 51, duration: 0.103s, episode steps:  27, steps per second: 262, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.587233, mae: 5.878956, mean_q: 11.386263, mean_eps: 0.949420\n",
      "  1214/20000: episode: 52, duration: 0.284s, episode steps:  76, steps per second: 267, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.297086, mae: 5.939650, mean_q: 11.558269, mean_eps: 0.947102\n",
      "  1239/20000: episode: 53, duration: 0.097s, episode steps:  25, steps per second: 258, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.160878, mae: 6.327218, mean_q: 12.489284, mean_eps: 0.944830\n",
      "  1249/20000: episode: 54, duration: 0.041s, episode steps:  10, steps per second: 242, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.395209, mae: 6.331136, mean_q: 12.376786, mean_eps: 0.944042\n",
      "  1259/20000: episode: 55, duration: 0.041s, episode steps:  10, steps per second: 243, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.717655, mae: 6.365290, mean_q: 12.414436, mean_eps: 0.943593\n",
      "  1274/20000: episode: 56, duration: 0.064s, episode steps:  15, steps per second: 235, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.438976, mae: 6.251176, mean_q: 12.163927, mean_eps: 0.943030\n",
      "  1286/20000: episode: 57, duration: 0.046s, episode steps:  12, steps per second: 259, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.512787, mae: 6.274045, mean_q: 12.377954, mean_eps: 0.942423\n",
      "  1296/20000: episode: 58, duration: 0.040s, episode steps:  10, steps per second: 251, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.176148, mae: 6.282634, mean_q: 12.307824, mean_eps: 0.941928\n",
      "  1312/20000: episode: 59, duration: 0.062s, episode steps:  16, steps per second: 258, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.842766, mae: 6.570551, mean_q: 12.653283, mean_eps: 0.941342\n",
      "  1340/20000: episode: 60, duration: 0.110s, episode steps:  28, steps per second: 255, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.680823, mae: 6.752071, mean_q: 13.268148, mean_eps: 0.940352\n",
      "  1363/20000: episode: 61, duration: 0.087s, episode steps:  23, steps per second: 265, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.696 [0.000, 1.000],  loss: 1.642503, mae: 6.747842, mean_q: 13.214140, mean_eps: 0.939205\n",
      "  1374/20000: episode: 62, duration: 0.049s, episode steps:  11, steps per second: 226, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.046366, mae: 6.770592, mean_q: 13.191960, mean_eps: 0.938440\n",
      "  1393/20000: episode: 63, duration: 0.076s, episode steps:  19, steps per second: 249, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 1.472771, mae: 6.699097, mean_q: 13.163334, mean_eps: 0.937765\n",
      "  1408/20000: episode: 64, duration: 0.065s, episode steps:  15, steps per second: 232, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.732937, mae: 6.811136, mean_q: 13.160466, mean_eps: 0.937000\n",
      "  1430/20000: episode: 65, duration: 0.083s, episode steps:  22, steps per second: 264, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 1.923932, mae: 7.049393, mean_q: 13.816865, mean_eps: 0.936167\n",
      "  1470/20000: episode: 66, duration: 0.155s, episode steps:  40, steps per second: 259, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.440957, mae: 6.971649, mean_q: 13.736822, mean_eps: 0.934773\n",
      "  1483/20000: episode: 67, duration: 0.054s, episode steps:  13, steps per second: 242, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.400527, mae: 7.052245, mean_q: 13.960288, mean_eps: 0.933580\n",
      "  1511/20000: episode: 68, duration: 0.128s, episode steps:  28, steps per second: 219, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.098735, mae: 7.139718, mean_q: 13.885547, mean_eps: 0.932658\n",
      "  1538/20000: episode: 69, duration: 0.109s, episode steps:  27, steps per second: 247, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 1.974199, mae: 7.390263, mean_q: 14.579232, mean_eps: 0.931420\n",
      "  1575/20000: episode: 70, duration: 0.138s, episode steps:  37, steps per second: 267, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.500064, mae: 7.370862, mean_q: 14.330524, mean_eps: 0.929980\n",
      "  1617/20000: episode: 71, duration: 0.168s, episode steps:  42, steps per second: 250, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.851873, mae: 7.609569, mean_q: 14.923405, mean_eps: 0.928203\n",
      "  1645/20000: episode: 72, duration: 0.106s, episode steps:  28, steps per second: 265, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.187447, mae: 7.818236, mean_q: 15.415865, mean_eps: 0.926628\n",
      "  1673/20000: episode: 73, duration: 0.105s, episode steps:  28, steps per second: 268, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 2.319879, mae: 7.923129, mean_q: 15.603835, mean_eps: 0.925367\n",
      "  1691/20000: episode: 74, duration: 0.072s, episode steps:  18, steps per second: 249, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 1.879338, mae: 7.806816, mean_q: 15.399725, mean_eps: 0.924333\n",
      "  1716/20000: episode: 75, duration: 0.120s, episode steps:  25, steps per second: 209, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.517032, mae: 7.996970, mean_q: 15.638006, mean_eps: 0.923365\n",
      "  1746/20000: episode: 76, duration: 0.126s, episode steps:  30, steps per second: 238, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 1.980188, mae: 8.173804, mean_q: 16.318550, mean_eps: 0.922128\n",
      "  1766/20000: episode: 77, duration: 0.077s, episode steps:  20, steps per second: 258, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.403329, mae: 8.118125, mean_q: 16.141472, mean_eps: 0.921002\n",
      "  1786/20000: episode: 78, duration: 0.075s, episode steps:  20, steps per second: 267, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.897436, mae: 8.173588, mean_q: 16.231838, mean_eps: 0.920103\n",
      "  1802/20000: episode: 79, duration: 0.065s, episode steps:  16, steps per second: 245, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 3.365817, mae: 8.054730, mean_q: 15.668205, mean_eps: 0.919293\n",
      "  1858/20000: episode: 80, duration: 0.210s, episode steps:  56, steps per second: 267, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.067354, mae: 8.402625, mean_q: 16.648578, mean_eps: 0.917673\n",
      "  1869/20000: episode: 81, duration: 0.047s, episode steps:  11, steps per second: 232, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.308113, mae: 8.372546, mean_q: 16.540013, mean_eps: 0.916165\n",
      "  1892/20000: episode: 82, duration: 0.085s, episode steps:  23, steps per second: 271, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 2.552755, mae: 8.441047, mean_q: 16.703378, mean_eps: 0.915400\n",
      "  1933/20000: episode: 83, duration: 0.158s, episode steps:  41, steps per second: 260, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.297164, mae: 8.698132, mean_q: 17.272009, mean_eps: 0.913960\n",
      "  1977/20000: episode: 84, duration: 0.163s, episode steps:  44, steps per second: 271, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.403040, mae: 8.801034, mean_q: 17.581429, mean_eps: 0.912047\n",
      "  1986/20000: episode: 85, duration: 0.038s, episode steps:   9, steps per second: 236, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 3.171274, mae: 8.679447, mean_q: 17.137897, mean_eps: 0.910855\n",
      "  1998/20000: episode: 86, duration: 0.051s, episode steps:  12, steps per second: 235, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.926013, mae: 8.787007, mean_q: 17.561106, mean_eps: 0.910383\n",
      "  2023/20000: episode: 87, duration: 0.093s, episode steps:  25, steps per second: 268, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.891972, mae: 9.302977, mean_q: 18.361401, mean_eps: 0.909550\n",
      "  2047/20000: episode: 88, duration: 0.101s, episode steps:  24, steps per second: 237, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.282898, mae: 9.316608, mean_q: 18.520578, mean_eps: 0.908447\n",
      "  2057/20000: episode: 89, duration: 0.051s, episode steps:  10, steps per second: 198, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.373593, mae: 9.158825, mean_q: 18.217259, mean_eps: 0.907682\n",
      "  2103/20000: episode: 90, duration: 0.177s, episode steps:  46, steps per second: 260, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.593913, mae: 9.237961, mean_q: 18.377167, mean_eps: 0.906423\n",
      "  2132/20000: episode: 91, duration: 0.116s, episode steps:  29, steps per second: 250, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.226724, mae: 9.652796, mean_q: 19.343891, mean_eps: 0.904735\n",
      "  2154/20000: episode: 92, duration: 0.085s, episode steps:  22, steps per second: 260, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 2.352833, mae: 9.594118, mean_q: 19.317876, mean_eps: 0.903587\n",
      "  2174/20000: episode: 93, duration: 0.074s, episode steps:  20, steps per second: 269, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.165312, mae: 9.555496, mean_q: 19.193955, mean_eps: 0.902643\n",
      "  2186/20000: episode: 94, duration: 0.049s, episode steps:  12, steps per second: 247, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 2.990315, mae: 9.588206, mean_q: 19.170682, mean_eps: 0.901922\n",
      "  2215/20000: episode: 95, duration: 0.117s, episode steps:  29, steps per second: 248, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.583219, mae: 9.768583, mean_q: 19.415702, mean_eps: 0.901000\n",
      "  2264/20000: episode: 96, duration: 0.187s, episode steps:  49, steps per second: 262, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 2.938702, mae: 10.151318, mean_q: 20.297810, mean_eps: 0.899245\n",
      "  2278/20000: episode: 97, duration: 0.057s, episode steps:  14, steps per second: 244, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.093185, mae: 10.081427, mean_q: 20.072473, mean_eps: 0.897827\n",
      "  2294/20000: episode: 98, duration: 0.062s, episode steps:  16, steps per second: 258, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 3.152147, mae: 10.098562, mean_q: 20.047767, mean_eps: 0.897153\n",
      "  2325/20000: episode: 99, duration: 0.117s, episode steps:  31, steps per second: 264, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.158101, mae: 10.364089, mean_q: 20.573981, mean_eps: 0.896095\n",
      "  2372/20000: episode: 100, duration: 0.183s, episode steps:  47, steps per second: 257, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.426 [0.000, 1.000],  loss: 2.477144, mae: 10.462197, mean_q: 21.049487, mean_eps: 0.894340\n",
      "  2396/20000: episode: 101, duration: 0.092s, episode steps:  24, steps per second: 261, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.663748, mae: 10.553250, mean_q: 21.142495, mean_eps: 0.892743\n",
      "  2428/20000: episode: 102, duration: 0.122s, episode steps:  32, steps per second: 263, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.458768, mae: 11.077549, mean_q: 22.264644, mean_eps: 0.891482\n",
      "  2442/20000: episode: 103, duration: 0.059s, episode steps:  14, steps per second: 236, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 2.140006, mae: 11.028456, mean_q: 22.297397, mean_eps: 0.890448\n",
      "  2480/20000: episode: 104, duration: 0.144s, episode steps:  38, steps per second: 264, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 2.222941, mae: 11.059965, mean_q: 22.392358, mean_eps: 0.889277\n",
      "  2494/20000: episode: 105, duration: 0.058s, episode steps:  14, steps per second: 241, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 3.151242, mae: 11.173876, mean_q: 22.371016, mean_eps: 0.888108\n",
      "  2518/20000: episode: 106, duration: 0.095s, episode steps:  24, steps per second: 253, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 4.109170, mae: 11.455715, mean_q: 22.835373, mean_eps: 0.887253\n",
      "  2532/20000: episode: 107, duration: 0.069s, episode steps:  14, steps per second: 202, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 3.192560, mae: 11.584491, mean_q: 23.048275, mean_eps: 0.886398\n",
      "  2549/20000: episode: 108, duration: 0.073s, episode steps:  17, steps per second: 232, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.808771, mae: 11.508692, mean_q: 23.194984, mean_eps: 0.885700\n",
      "  2570/20000: episode: 109, duration: 0.085s, episode steps:  21, steps per second: 246, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.515516, mae: 11.450268, mean_q: 23.151809, mean_eps: 0.884845\n",
      "  2607/20000: episode: 110, duration: 0.163s, episode steps:  37, steps per second: 228, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.826198, mae: 11.610856, mean_q: 23.572736, mean_eps: 0.883540\n",
      "  2644/20000: episode: 111, duration: 0.145s, episode steps:  37, steps per second: 255, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.595 [0.000, 1.000],  loss: 2.525851, mae: 12.206938, mean_q: 24.713943, mean_eps: 0.881875\n",
      "  2675/20000: episode: 112, duration: 0.124s, episode steps:  31, steps per second: 250, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 3.947049, mae: 12.184594, mean_q: 24.498859, mean_eps: 0.880345\n",
      "  2714/20000: episode: 113, duration: 0.148s, episode steps:  39, steps per second: 263, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  loss: 2.655301, mae: 12.312349, mean_q: 24.916029, mean_eps: 0.878770\n",
      "  2732/20000: episode: 114, duration: 0.070s, episode steps:  18, steps per second: 258, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 2.378947, mae: 12.737483, mean_q: 25.795615, mean_eps: 0.877488\n",
      "  2751/20000: episode: 115, duration: 0.077s, episode steps:  19, steps per second: 246, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 2.810185, mae: 12.781446, mean_q: 25.997915, mean_eps: 0.876655\n",
      "  2806/20000: episode: 116, duration: 0.207s, episode steps:  55, steps per second: 266, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.901336, mae: 12.644425, mean_q: 25.437202, mean_eps: 0.874990\n",
      "  2839/20000: episode: 117, duration: 0.126s, episode steps:  33, steps per second: 262, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.265526, mae: 12.903621, mean_q: 25.999897, mean_eps: 0.873010\n",
      "  2857/20000: episode: 118, duration: 0.073s, episode steps:  18, steps per second: 246, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 3.593437, mae: 12.836675, mean_q: 26.165887, mean_eps: 0.871863\n",
      "  2872/20000: episode: 119, duration: 0.063s, episode steps:  15, steps per second: 237, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.922589, mae: 13.069284, mean_q: 26.511261, mean_eps: 0.871120\n",
      "  2882/20000: episode: 120, duration: 0.042s, episode steps:  10, steps per second: 236, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 3.720318, mae: 13.019738, mean_q: 26.515964, mean_eps: 0.870557\n",
      "  2894/20000: episode: 121, duration: 0.047s, episode steps:  12, steps per second: 257, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.050077, mae: 13.095910, mean_q: 26.523990, mean_eps: 0.870062\n",
      "  2906/20000: episode: 122, duration: 0.054s, episode steps:  12, steps per second: 224, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 4.679505, mae: 13.099704, mean_q: 26.282341, mean_eps: 0.869522\n",
      "  2940/20000: episode: 123, duration: 0.150s, episode steps:  34, steps per second: 227, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.505672, mae: 13.448457, mean_q: 27.130642, mean_eps: 0.868487\n",
      "  2998/20000: episode: 124, duration: 0.226s, episode steps:  58, steps per second: 256, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 3.905973, mae: 13.389837, mean_q: 27.023041, mean_eps: 0.866418\n",
      "  3031/20000: episode: 125, duration: 0.130s, episode steps:  33, steps per second: 254, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.435399, mae: 13.834006, mean_q: 27.996957, mean_eps: 0.864370\n",
      "  3083/20000: episode: 126, duration: 0.199s, episode steps:  52, steps per second: 261, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.287775, mae: 13.867443, mean_q: 27.922409, mean_eps: 0.862458\n",
      "  3095/20000: episode: 127, duration: 0.049s, episode steps:  12, steps per second: 245, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 3.306672, mae: 13.986353, mean_q: 28.272440, mean_eps: 0.861017\n",
      "  3115/20000: episode: 128, duration: 0.079s, episode steps:  20, steps per second: 254, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.115751, mae: 13.989723, mean_q: 28.166261, mean_eps: 0.860298\n",
      "  3125/20000: episode: 129, duration: 0.045s, episode steps:  10, steps per second: 222, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.507048, mae: 14.220621, mean_q: 28.956681, mean_eps: 0.859623\n",
      "  3137/20000: episode: 130, duration: 0.057s, episode steps:  12, steps per second: 211, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 4.398417, mae: 13.927353, mean_q: 28.144986, mean_eps: 0.859127\n",
      "  3195/20000: episode: 131, duration: 0.223s, episode steps:  58, steps per second: 261, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 3.235117, mae: 14.215827, mean_q: 28.830856, mean_eps: 0.857553\n",
      "  3210/20000: episode: 132, duration: 0.065s, episode steps:  15, steps per second: 232, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.502515, mae: 14.361532, mean_q: 29.001380, mean_eps: 0.855910\n",
      "  3222/20000: episode: 133, duration: 0.049s, episode steps:  12, steps per second: 247, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.661471, mae: 14.841297, mean_q: 29.957688, mean_eps: 0.855303\n",
      "  3245/20000: episode: 134, duration: 0.089s, episode steps:  23, steps per second: 258, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.145328, mae: 14.686017, mean_q: 29.833872, mean_eps: 0.854515\n",
      "  3302/20000: episode: 135, duration: 0.217s, episode steps:  57, steps per second: 263, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.986788, mae: 14.740609, mean_q: 29.976938, mean_eps: 0.852715\n",
      "  3330/20000: episode: 136, duration: 0.111s, episode steps:  28, steps per second: 252, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.434664, mae: 15.307179, mean_q: 31.373264, mean_eps: 0.850802\n",
      "  3344/20000: episode: 137, duration: 0.055s, episode steps:  14, steps per second: 256, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.639462, mae: 15.117294, mean_q: 30.560534, mean_eps: 0.849858\n",
      "  3364/20000: episode: 138, duration: 0.079s, episode steps:  20, steps per second: 253, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 3.137104, mae: 15.555958, mean_q: 31.640256, mean_eps: 0.849093\n",
      "  3402/20000: episode: 139, duration: 0.145s, episode steps:  38, steps per second: 262, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.054762, mae: 15.190247, mean_q: 31.026967, mean_eps: 0.847788\n",
      "  3433/20000: episode: 140, duration: 0.118s, episode steps:  31, steps per second: 263, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.641854, mae: 15.655490, mean_q: 31.780003, mean_eps: 0.846235\n",
      "  3464/20000: episode: 141, duration: 0.118s, episode steps:  31, steps per second: 263, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 3.436642, mae: 15.806904, mean_q: 32.132401, mean_eps: 0.844840\n",
      "  3506/20000: episode: 142, duration: 0.161s, episode steps:  42, steps per second: 260, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.595 [0.000, 1.000],  loss: 3.465692, mae: 15.674564, mean_q: 32.027233, mean_eps: 0.843198\n",
      "  3570/20000: episode: 143, duration: 0.239s, episode steps:  64, steps per second: 267, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.237801, mae: 16.159136, mean_q: 33.064367, mean_eps: 0.840812\n",
      "  3635/20000: episode: 144, duration: 0.251s, episode steps:  65, steps per second: 259, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.350067, mae: 16.473207, mean_q: 33.733064, mean_eps: 0.837910\n",
      "  3670/20000: episode: 145, duration: 0.133s, episode steps:  35, steps per second: 263, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.105372, mae: 16.602632, mean_q: 33.905354, mean_eps: 0.835660\n",
      "  3686/20000: episode: 146, duration: 0.069s, episode steps:  16, steps per second: 230, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 2.611441, mae: 16.907983, mean_q: 34.734277, mean_eps: 0.834512\n",
      "  3737/20000: episode: 147, duration: 0.208s, episode steps:  51, steps per second: 245, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.820006, mae: 16.948567, mean_q: 34.558594, mean_eps: 0.833005\n",
      "  3762/20000: episode: 148, duration: 0.094s, episode steps:  25, steps per second: 265, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.829695, mae: 17.105799, mean_q: 34.836507, mean_eps: 0.831295\n",
      "  3833/20000: episode: 149, duration: 0.278s, episode steps:  71, steps per second: 255, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.022489, mae: 17.519244, mean_q: 35.759774, mean_eps: 0.829135\n",
      "  3862/20000: episode: 150, duration: 0.113s, episode steps:  29, steps per second: 256, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 3.217989, mae: 17.635816, mean_q: 36.124139, mean_eps: 0.826885\n",
      "  3886/20000: episode: 151, duration: 0.092s, episode steps:  24, steps per second: 260, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.702983, mae: 17.809534, mean_q: 36.336112, mean_eps: 0.825693\n",
      "  3930/20000: episode: 152, duration: 0.169s, episode steps:  44, steps per second: 260, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 3.133960, mae: 18.263878, mean_q: 37.390343, mean_eps: 0.824162\n",
      "  4005/20000: episode: 153, duration: 0.281s, episode steps:  75, steps per second: 267, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.546511, mae: 18.485757, mean_q: 37.734952, mean_eps: 0.821485\n",
      "  4053/20000: episode: 154, duration: 0.176s, episode steps:  48, steps per second: 272, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.974135, mae: 18.682221, mean_q: 38.221664, mean_eps: 0.818717\n",
      "  4104/20000: episode: 155, duration: 0.229s, episode steps:  51, steps per second: 223, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.675971, mae: 18.331519, mean_q: 37.733901, mean_eps: 0.816490\n",
      "  4118/20000: episode: 156, duration: 0.056s, episode steps:  14, steps per second: 252, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 3.272644, mae: 19.496213, mean_q: 39.981467, mean_eps: 0.815028\n",
      "  4161/20000: episode: 157, duration: 0.162s, episode steps:  43, steps per second: 265, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 5.483079, mae: 19.172747, mean_q: 39.140116, mean_eps: 0.813745\n",
      "  4186/20000: episode: 158, duration: 0.097s, episode steps:  25, steps per second: 257, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 5.931374, mae: 18.770315, mean_q: 38.260497, mean_eps: 0.812215\n",
      "  4214/20000: episode: 159, duration: 0.105s, episode steps:  28, steps per second: 266, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5.089867, mae: 19.257939, mean_q: 39.099517, mean_eps: 0.811022\n",
      "  4259/20000: episode: 160, duration: 0.203s, episode steps:  45, steps per second: 222, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.449442, mae: 19.434627, mean_q: 39.810995, mean_eps: 0.809380\n",
      "  4334/20000: episode: 161, duration: 0.291s, episode steps:  75, steps per second: 258, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.047348, mae: 19.907017, mean_q: 40.733694, mean_eps: 0.806680\n",
      "  4348/20000: episode: 162, duration: 0.056s, episode steps:  14, steps per second: 250, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.841104, mae: 20.336201, mean_q: 42.023421, mean_eps: 0.804678\n",
      "  4363/20000: episode: 163, duration: 0.061s, episode steps:  15, steps per second: 244, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5.502144, mae: 20.226681, mean_q: 41.510448, mean_eps: 0.804025\n",
      "  4381/20000: episode: 164, duration: 0.070s, episode steps:  18, steps per second: 258, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.395560, mae: 20.868271, mean_q: 42.753788, mean_eps: 0.803282\n",
      "  4418/20000: episode: 165, duration: 0.143s, episode steps:  37, steps per second: 258, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 3.152615, mae: 20.725685, mean_q: 42.818196, mean_eps: 0.802045\n",
      "  4436/20000: episode: 166, duration: 0.070s, episode steps:  18, steps per second: 256, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 5.194739, mae: 20.379310, mean_q: 42.194057, mean_eps: 0.800808\n",
      "  4501/20000: episode: 167, duration: 0.244s, episode steps:  65, steps per second: 267, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.168237, mae: 21.096129, mean_q: 43.511383, mean_eps: 0.798940\n",
      "  4529/20000: episode: 168, duration: 0.105s, episode steps:  28, steps per second: 266, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 5.726557, mae: 21.136745, mean_q: 43.540321, mean_eps: 0.796848\n",
      "  4547/20000: episode: 169, duration: 0.070s, episode steps:  18, steps per second: 259, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.399274, mae: 21.821932, mean_q: 45.143727, mean_eps: 0.795812\n",
      "  4585/20000: episode: 170, duration: 0.147s, episode steps:  38, steps per second: 259, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 5.165435, mae: 21.679498, mean_q: 44.517898, mean_eps: 0.794553\n",
      "  4605/20000: episode: 171, duration: 0.077s, episode steps:  20, steps per second: 259, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 6.486841, mae: 21.808050, mean_q: 44.522254, mean_eps: 0.793247\n",
      "  4625/20000: episode: 172, duration: 0.076s, episode steps:  20, steps per second: 261, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.851086, mae: 22.448936, mean_q: 46.071290, mean_eps: 0.792347\n",
      "  4671/20000: episode: 173, duration: 0.177s, episode steps:  46, steps per second: 260, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 4.748585, mae: 22.176051, mean_q: 45.509790, mean_eps: 0.790863\n",
      "  4687/20000: episode: 174, duration: 0.062s, episode steps:  16, steps per second: 259, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 4.631428, mae: 22.557765, mean_q: 46.319340, mean_eps: 0.789467\n",
      "  4732/20000: episode: 175, duration: 0.168s, episode steps:  45, steps per second: 268, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 4.345032, mae: 22.052741, mean_q: 45.496490, mean_eps: 0.788095\n",
      "  4743/20000: episode: 176, duration: 0.049s, episode steps:  11, steps per second: 223, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.706110, mae: 23.123020, mean_q: 47.870004, mean_eps: 0.786835\n",
      "  4807/20000: episode: 177, duration: 0.256s, episode steps:  64, steps per second: 250, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 5.178477, mae: 22.517898, mean_q: 46.558418, mean_eps: 0.785147\n",
      "  4822/20000: episode: 178, duration: 0.065s, episode steps:  15, steps per second: 230, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.724351, mae: 23.160685, mean_q: 47.450484, mean_eps: 0.783370\n",
      "  4836/20000: episode: 179, duration: 0.055s, episode steps:  14, steps per second: 256, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 5.901626, mae: 23.224013, mean_q: 47.679958, mean_eps: 0.782718\n",
      "  4901/20000: episode: 180, duration: 0.245s, episode steps:  65, steps per second: 266, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.479637, mae: 22.983673, mean_q: 47.209197, mean_eps: 0.780940\n",
      "  4912/20000: episode: 181, duration: 0.044s, episode steps:  11, steps per second: 252, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 8.137959, mae: 23.606777, mean_q: 48.285956, mean_eps: 0.779230\n",
      "  4927/20000: episode: 182, duration: 0.069s, episode steps:  15, steps per second: 216, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5.427619, mae: 23.574102, mean_q: 48.586061, mean_eps: 0.778645\n",
      "  4982/20000: episode: 183, duration: 0.213s, episode steps:  55, steps per second: 259, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.555127, mae: 23.480731, mean_q: 48.549833, mean_eps: 0.777070\n",
      "  5071/20000: episode: 184, duration: 0.336s, episode steps:  89, steps per second: 265, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 5.294233, mae: 24.197717, mean_q: 49.947759, mean_eps: 0.773830\n",
      "  5090/20000: episode: 185, duration: 0.074s, episode steps:  19, steps per second: 257, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 9.284917, mae: 23.854880, mean_q: 49.181118, mean_eps: 0.771400\n",
      "  5147/20000: episode: 186, duration: 0.217s, episode steps:  57, steps per second: 262, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.858740, mae: 24.790699, mean_q: 50.852819, mean_eps: 0.769690\n",
      "  5163/20000: episode: 187, duration: 0.061s, episode steps:  16, steps per second: 260, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 3.209944, mae: 24.589716, mean_q: 50.698707, mean_eps: 0.768047\n",
      "  5227/20000: episode: 188, duration: 0.246s, episode steps:  64, steps per second: 260, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.054173, mae: 24.921488, mean_q: 51.238479, mean_eps: 0.766247\n",
      "  5292/20000: episode: 189, duration: 0.255s, episode steps:  65, steps per second: 254, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.860640, mae: 25.396427, mean_q: 52.046950, mean_eps: 0.763345\n",
      "  5314/20000: episode: 190, duration: 0.087s, episode steps:  22, steps per second: 252, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 7.528857, mae: 25.973275, mean_q: 53.190529, mean_eps: 0.761387\n",
      "  5356/20000: episode: 191, duration: 0.169s, episode steps:  42, steps per second: 249, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 8.935629, mae: 25.857694, mean_q: 52.897886, mean_eps: 0.759947\n",
      "  5400/20000: episode: 192, duration: 0.177s, episode steps:  44, steps per second: 248, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.389103, mae: 25.981800, mean_q: 53.349641, mean_eps: 0.758012\n",
      "  5438/20000: episode: 193, duration: 0.167s, episode steps:  38, steps per second: 228, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 6.619885, mae: 26.436135, mean_q: 54.420556, mean_eps: 0.756167\n",
      "  5467/20000: episode: 194, duration: 0.111s, episode steps:  29, steps per second: 262, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 6.688841, mae: 27.158936, mean_q: 55.678407, mean_eps: 0.754660\n",
      "  5490/20000: episode: 195, duration: 0.090s, episode steps:  23, steps per second: 256, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 3.542262, mae: 26.641647, mean_q: 54.783110, mean_eps: 0.753490\n",
      "  5520/20000: episode: 196, duration: 0.119s, episode steps:  30, steps per second: 252, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 5.645364, mae: 26.847864, mean_q: 55.107389, mean_eps: 0.752297\n",
      "  5546/20000: episode: 197, duration: 0.099s, episode steps:  26, steps per second: 264, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.605947, mae: 26.718193, mean_q: 54.867872, mean_eps: 0.751038\n",
      "  5593/20000: episode: 198, duration: 0.181s, episode steps:  47, steps per second: 260, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 6.424389, mae: 27.104664, mean_q: 55.585863, mean_eps: 0.749395\n",
      "  5632/20000: episode: 199, duration: 0.147s, episode steps:  39, steps per second: 266, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 7.183362, mae: 27.676909, mean_q: 56.657990, mean_eps: 0.747460\n",
      "  5670/20000: episode: 200, duration: 0.151s, episode steps:  38, steps per second: 252, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.452510, mae: 28.205540, mean_q: 57.838080, mean_eps: 0.745727\n",
      "  5730/20000: episode: 201, duration: 0.224s, episode steps:  60, steps per second: 268, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.964257, mae: 27.964980, mean_q: 57.404911, mean_eps: 0.743523\n",
      "  5786/20000: episode: 202, duration: 0.238s, episode steps:  56, steps per second: 235, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 5.848183, mae: 28.033487, mean_q: 57.885394, mean_eps: 0.740913\n",
      "  5900/20000: episode: 203, duration: 0.433s, episode steps: 114, steps per second: 263, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.803371, mae: 28.829524, mean_q: 59.299547, mean_eps: 0.737087\n",
      "  5923/20000: episode: 204, duration: 0.104s, episode steps:  23, steps per second: 222, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 8.649100, mae: 29.720239, mean_q: 60.886927, mean_eps: 0.734005\n",
      "  5939/20000: episode: 205, duration: 0.064s, episode steps:  16, steps per second: 249, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 6.963091, mae: 29.606109, mean_q: 61.253431, mean_eps: 0.733127\n",
      "  5965/20000: episode: 206, duration: 0.103s, episode steps:  26, steps per second: 252, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.021416, mae: 29.076461, mean_q: 59.945743, mean_eps: 0.732182\n",
      "  6071/20000: episode: 207, duration: 0.393s, episode steps: 106, steps per second: 269, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 7.557255, mae: 29.856715, mean_q: 61.457216, mean_eps: 0.729212\n",
      "  6089/20000: episode: 208, duration: 0.070s, episode steps:  18, steps per second: 258, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 6.957500, mae: 30.508756, mean_q: 62.467684, mean_eps: 0.726422\n",
      "  6244/20000: episode: 209, duration: 0.575s, episode steps: 155, steps per second: 269, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 7.937070, mae: 30.988555, mean_q: 63.578682, mean_eps: 0.722530\n",
      "  6286/20000: episode: 210, duration: 0.158s, episode steps:  42, steps per second: 267, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 9.901109, mae: 31.669285, mean_q: 64.671124, mean_eps: 0.718098\n",
      "  6300/20000: episode: 211, duration: 0.056s, episode steps:  14, steps per second: 250, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 7.141006, mae: 32.105389, mean_q: 66.088009, mean_eps: 0.716838\n",
      "  6352/20000: episode: 212, duration: 0.197s, episode steps:  52, steps per second: 263, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 9.279052, mae: 32.500047, mean_q: 66.339118, mean_eps: 0.715352\n",
      "  6394/20000: episode: 213, duration: 0.164s, episode steps:  42, steps per second: 257, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.789715, mae: 32.608459, mean_q: 66.397547, mean_eps: 0.713237\n",
      "  6480/20000: episode: 214, duration: 0.341s, episode steps:  86, steps per second: 252, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.832233, mae: 32.810547, mean_q: 66.960120, mean_eps: 0.710357\n",
      "  6504/20000: episode: 215, duration: 0.100s, episode steps:  24, steps per second: 241, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 8.965356, mae: 33.343139, mean_q: 68.068071, mean_eps: 0.707883\n",
      "  6577/20000: episode: 216, duration: 0.273s, episode steps:  73, steps per second: 267, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 9.850386, mae: 33.651233, mean_q: 68.924675, mean_eps: 0.705700\n",
      "  6614/20000: episode: 217, duration: 0.157s, episode steps:  37, steps per second: 235, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 9.218165, mae: 34.205711, mean_q: 70.043055, mean_eps: 0.703225\n",
      "  6636/20000: episode: 218, duration: 0.088s, episode steps:  22, steps per second: 251, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 7.195545, mae: 34.412545, mean_q: 70.577893, mean_eps: 0.701898\n",
      "  6766/20000: episode: 219, duration: 0.508s, episode steps: 130, steps per second: 256, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 10.627210, mae: 34.691659, mean_q: 71.204163, mean_eps: 0.698477\n",
      "  6789/20000: episode: 220, duration: 0.088s, episode steps:  23, steps per second: 262, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 12.113605, mae: 34.871887, mean_q: 71.435750, mean_eps: 0.695035\n",
      "  6820/20000: episode: 221, duration: 0.115s, episode steps:  31, steps per second: 268, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 7.438399, mae: 36.388889, mean_q: 74.503799, mean_eps: 0.693820\n",
      "  6833/20000: episode: 222, duration: 0.058s, episode steps:  13, steps per second: 224, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 11.018917, mae: 35.587026, mean_q: 73.248958, mean_eps: 0.692830\n",
      "  6932/20000: episode: 223, duration: 0.368s, episode steps:  99, steps per second: 269, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 11.630456, mae: 36.612736, mean_q: 74.969854, mean_eps: 0.690310\n",
      "  6987/20000: episode: 224, duration: 0.205s, episode steps:  55, steps per second: 269, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 12.818523, mae: 37.188447, mean_q: 75.913275, mean_eps: 0.686845\n",
      "  7127/20000: episode: 225, duration: 0.537s, episode steps: 140, steps per second: 261, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 13.617323, mae: 37.700376, mean_q: 77.047008, mean_eps: 0.682457\n",
      "  7142/20000: episode: 226, duration: 0.057s, episode steps:  15, steps per second: 263, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 13.227227, mae: 37.068731, mean_q: 75.689398, mean_eps: 0.678970\n",
      "  7284/20000: episode: 227, duration: 0.526s, episode steps: 142, steps per second: 270, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 10.752267, mae: 38.224635, mean_q: 78.859841, mean_eps: 0.675438\n",
      "  7299/20000: episode: 228, duration: 0.057s, episode steps:  15, steps per second: 264, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 11.142238, mae: 38.684674, mean_q: 79.542263, mean_eps: 0.671905\n",
      "  7394/20000: episode: 229, duration: 0.352s, episode steps:  95, steps per second: 270, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 14.116199, mae: 39.282922, mean_q: 80.564270, mean_eps: 0.669430\n",
      "  7492/20000: episode: 230, duration: 0.371s, episode steps:  98, steps per second: 264, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 13.293117, mae: 40.011094, mean_q: 82.191410, mean_eps: 0.665087\n",
      "  7510/20000: episode: 231, duration: 0.070s, episode steps:  18, steps per second: 258, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.862091, mae: 40.657712, mean_q: 83.310162, mean_eps: 0.662478\n",
      "  7551/20000: episode: 232, duration: 0.160s, episode steps:  41, steps per second: 256, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 15.712712, mae: 40.428180, mean_q: 83.281650, mean_eps: 0.661150\n",
      "  7572/20000: episode: 233, duration: 0.083s, episode steps:  21, steps per second: 254, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 14.433657, mae: 41.503222, mean_q: 85.139392, mean_eps: 0.659755\n",
      "  7618/20000: episode: 234, duration: 0.195s, episode steps:  46, steps per second: 236, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 11.742718, mae: 40.453158, mean_q: 83.181857, mean_eps: 0.658247\n",
      "  7755/20000: episode: 235, duration: 0.518s, episode steps: 137, steps per second: 265, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 16.724971, mae: 41.673793, mean_q: 85.314397, mean_eps: 0.654130\n",
      "  7839/20000: episode: 236, duration: 0.358s, episode steps:  84, steps per second: 234, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 12.311830, mae: 42.893128, mean_q: 87.865932, mean_eps: 0.649157\n",
      "  7983/20000: episode: 237, duration: 0.538s, episode steps: 144, steps per second: 267, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 14.991396, mae: 43.045349, mean_q: 88.348519, mean_eps: 0.644027\n",
      "  8051/20000: episode: 238, duration: 0.260s, episode steps:  68, steps per second: 262, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.613503, mae: 43.136925, mean_q: 88.482536, mean_eps: 0.639257\n",
      "  8062/20000: episode: 239, duration: 0.044s, episode steps:  11, steps per second: 247, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 12.707999, mae: 44.939038, mean_q: 91.845612, mean_eps: 0.637480\n",
      "  8102/20000: episode: 240, duration: 0.153s, episode steps:  40, steps per second: 261, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 12.172729, mae: 43.516995, mean_q: 89.125144, mean_eps: 0.636332\n",
      "  8302/20000: episode: 241, duration: 0.763s, episode steps: 200, steps per second: 262, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.103961, mae: 45.097156, mean_q: 92.294819, mean_eps: 0.630933\n",
      "  8317/20000: episode: 242, duration: 0.059s, episode steps:  15, steps per second: 256, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 14.092285, mae: 45.516556, mean_q: 93.238563, mean_eps: 0.626095\n",
      "  8494/20000: episode: 243, duration: 0.659s, episode steps: 177, steps per second: 269, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 18.914008, mae: 46.481280, mean_q: 94.991607, mean_eps: 0.621775\n",
      "  8547/20000: episode: 244, duration: 0.198s, episode steps:  53, steps per second: 267, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 15.282342, mae: 46.868959, mean_q: 96.193946, mean_eps: 0.616600\n",
      "  8611/20000: episode: 245, duration: 0.249s, episode steps:  64, steps per second: 257, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 16.636853, mae: 47.153918, mean_q: 96.665218, mean_eps: 0.613967\n",
      "  8808/20000: episode: 246, duration: 0.753s, episode steps: 197, steps per second: 262, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 20.656487, mae: 48.042452, mean_q: 98.246306, mean_eps: 0.608095\n",
      "  8849/20000: episode: 247, duration: 0.151s, episode steps:  41, steps per second: 272, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 30.094671, mae: 49.364280, mean_q: 100.391918, mean_eps: 0.602740\n",
      "  8957/20000: episode: 248, duration: 0.402s, episode steps: 108, steps per second: 269, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 25.331049, mae: 49.626974, mean_q: 101.497657, mean_eps: 0.599388\n",
      "  9051/20000: episode: 249, duration: 0.357s, episode steps:  94, steps per second: 263, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 23.457239, mae: 50.065452, mean_q: 102.464985, mean_eps: 0.594843\n",
      "  9119/20000: episode: 250, duration: 0.283s, episode steps:  68, steps per second: 240, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.299125, mae: 51.319691, mean_q: 104.944890, mean_eps: 0.591197\n",
      "  9205/20000: episode: 251, duration: 0.328s, episode steps:  86, steps per second: 262, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 21.986264, mae: 51.056314, mean_q: 104.711167, mean_eps: 0.587732\n",
      "  9324/20000: episode: 252, duration: 0.471s, episode steps: 119, steps per second: 253, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 28.260736, mae: 51.605315, mean_q: 105.469043, mean_eps: 0.583120\n",
      "  9447/20000: episode: 253, duration: 0.470s, episode steps: 123, steps per second: 262, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 24.733005, mae: 52.116825, mean_q: 106.777112, mean_eps: 0.577675\n",
      "  9525/20000: episode: 254, duration: 0.330s, episode steps:  78, steps per second: 236, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 26.429022, mae: 52.993209, mean_q: 108.341290, mean_eps: 0.573153\n",
      "  9566/20000: episode: 255, duration: 0.155s, episode steps:  41, steps per second: 265, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.415 [0.000, 1.000],  loss: 27.957336, mae: 53.111763, mean_q: 108.535114, mean_eps: 0.570475\n",
      "  9741/20000: episode: 256, duration: 0.649s, episode steps: 175, steps per second: 270, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 27.335258, mae: 54.010927, mean_q: 110.485217, mean_eps: 0.565615\n",
      "  9832/20000: episode: 257, duration: 0.336s, episode steps:  91, steps per second: 271, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 26.112213, mae: 55.192114, mean_q: 112.723695, mean_eps: 0.559630\n",
      "  9864/20000: episode: 258, duration: 0.131s, episode steps:  32, steps per second: 244, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 29.819995, mae: 55.319422, mean_q: 112.743010, mean_eps: 0.556863\n",
      " 10005/20000: episode: 259, duration: 0.536s, episode steps: 141, steps per second: 263, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 22.919663, mae: 55.913149, mean_q: 114.276257, mean_eps: 0.552970\n",
      " 10096/20000: episode: 260, duration: 0.360s, episode steps:  91, steps per second: 252, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 23.712471, mae: 56.197779, mean_q: 114.808414, mean_eps: 0.547750\n",
      " 10240/20000: episode: 261, duration: 0.581s, episode steps: 144, steps per second: 248, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 21.726573, mae: 57.670318, mean_q: 117.967233, mean_eps: 0.542462\n",
      " 10324/20000: episode: 262, duration: 0.332s, episode steps:  84, steps per second: 253, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 25.810261, mae: 58.402111, mean_q: 119.244461, mean_eps: 0.537332\n",
      " 10444/20000: episode: 263, duration: 0.550s, episode steps: 120, steps per second: 218, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 25.229361, mae: 58.579989, mean_q: 120.027817, mean_eps: 0.532742\n",
      " 10644/20000: episode: 264, duration: 0.751s, episode steps: 200, steps per second: 266, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 23.921650, mae: 59.361931, mean_q: 121.357776, mean_eps: 0.525543\n",
      " 10690/20000: episode: 265, duration: 0.174s, episode steps:  46, steps per second: 265, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 31.620222, mae: 60.139038, mean_q: 122.785071, mean_eps: 0.520008\n",
      " 10890/20000: episode: 266, duration: 0.770s, episode steps: 200, steps per second: 260, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 25.356249, mae: 61.663675, mean_q: 126.172220, mean_eps: 0.514472\n",
      " 11090/20000: episode: 267, duration: 0.787s, episode steps: 200, steps per second: 254, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 29.898374, mae: 63.062018, mean_q: 128.659045, mean_eps: 0.505472\n",
      " 11290/20000: episode: 268, duration: 0.751s, episode steps: 200, steps per second: 266, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 34.371487, mae: 63.877010, mean_q: 130.553188, mean_eps: 0.496472\n",
      " 11490/20000: episode: 269, duration: 0.744s, episode steps: 200, steps per second: 269, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 35.054909, mae: 64.887640, mean_q: 132.456820, mean_eps: 0.487472\n",
      " 11690/20000: episode: 270, duration: 0.794s, episode steps: 200, steps per second: 252, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.847561, mae: 65.519886, mean_q: 133.526326, mean_eps: 0.478472\n",
      " 11890/20000: episode: 271, duration: 0.741s, episode steps: 200, steps per second: 270, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 33.041918, mae: 65.771601, mean_q: 134.063426, mean_eps: 0.469472\n",
      " 11909/20000: episode: 272, duration: 0.074s, episode steps:  19, steps per second: 256, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 16.947051, mae: 66.330258, mean_q: 135.220261, mean_eps: 0.464545\n",
      " 12075/20000: episode: 273, duration: 0.615s, episode steps: 166, steps per second: 270, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 29.623311, mae: 66.828539, mean_q: 136.524619, mean_eps: 0.460383\n",
      " 12192/20000: episode: 274, duration: 0.460s, episode steps: 117, steps per second: 255, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 45.386239, mae: 67.099219, mean_q: 136.970349, mean_eps: 0.454015\n",
      " 12243/20000: episode: 275, duration: 0.193s, episode steps:  51, steps per second: 264, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 47.220091, mae: 67.157739, mean_q: 136.979837, mean_eps: 0.450235\n",
      " 12443/20000: episode: 276, duration: 0.736s, episode steps: 200, steps per second: 272, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 42.135796, mae: 68.200933, mean_q: 138.975379, mean_eps: 0.444588\n",
      " 12478/20000: episode: 277, duration: 0.139s, episode steps:  35, steps per second: 252, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 38.596336, mae: 68.884440, mean_q: 140.475574, mean_eps: 0.439300\n",
      " 12678/20000: episode: 278, duration: 0.741s, episode steps: 200, steps per second: 270, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 36.770625, mae: 69.401840, mean_q: 141.509257, mean_eps: 0.434012\n",
      " 12856/20000: episode: 279, duration: 0.707s, episode steps: 178, steps per second: 252, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 30.097507, mae: 70.291865, mean_q: 143.317923, mean_eps: 0.425507\n",
      " 13048/20000: episode: 280, duration: 0.732s, episode steps: 192, steps per second: 262, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 43.456319, mae: 70.387451, mean_q: 143.295780, mean_eps: 0.417182\n",
      " 13248/20000: episode: 281, duration: 0.752s, episode steps: 200, steps per second: 266, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 45.782432, mae: 70.957743, mean_q: 144.538488, mean_eps: 0.408362\n",
      " 13446/20000: episode: 282, duration: 0.752s, episode steps: 198, steps per second: 263, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 43.748193, mae: 73.019860, mean_q: 149.040292, mean_eps: 0.399408\n",
      " 13626/20000: episode: 283, duration: 0.672s, episode steps: 180, steps per second: 268, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 56.869817, mae: 74.120394, mean_q: 150.502306, mean_eps: 0.390902\n",
      " 13826/20000: episode: 284, duration: 0.773s, episode steps: 200, steps per second: 259, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 51.287348, mae: 74.689310, mean_q: 151.511503, mean_eps: 0.382352\n",
      " 14026/20000: episode: 285, duration: 0.773s, episode steps: 200, steps per second: 259, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 52.654955, mae: 75.334509, mean_q: 153.232685, mean_eps: 0.373353\n",
      " 14226/20000: episode: 286, duration: 0.770s, episode steps: 200, steps per second: 260, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 41.660561, mae: 75.972293, mean_q: 154.560104, mean_eps: 0.364352\n",
      " 14294/20000: episode: 287, duration: 0.259s, episode steps:  68, steps per second: 263, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 43.314546, mae: 76.749991, mean_q: 156.047761, mean_eps: 0.358323\n",
      " 14404/20000: episode: 288, duration: 0.439s, episode steps: 110, steps per second: 251, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 58.552819, mae: 76.756873, mean_q: 155.544804, mean_eps: 0.354317\n",
      " 14604/20000: episode: 289, duration: 0.764s, episode steps: 200, steps per second: 262, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 64.617338, mae: 77.313719, mean_q: 156.847696, mean_eps: 0.347342\n",
      " 14723/20000: episode: 290, duration: 0.445s, episode steps: 119, steps per second: 268, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 38.553445, mae: 77.584459, mean_q: 157.662983, mean_eps: 0.340165\n",
      " 14904/20000: episode: 291, duration: 0.759s, episode steps: 181, steps per second: 239, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 38.983632, mae: 77.903821, mean_q: 158.703821, mean_eps: 0.333415\n",
      " 15079/20000: episode: 292, duration: 0.713s, episode steps: 175, steps per second: 245, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 40.711757, mae: 78.961082, mean_q: 160.681610, mean_eps: 0.325405\n",
      " 15115/20000: episode: 293, duration: 0.147s, episode steps:  36, steps per second: 246, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 61.063149, mae: 79.250959, mean_q: 160.637373, mean_eps: 0.320657\n",
      " 15315/20000: episode: 294, duration: 0.801s, episode steps: 200, steps per second: 250, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 46.280652, mae: 78.997901, mean_q: 160.655530, mean_eps: 0.315347\n",
      " 15460/20000: episode: 295, duration: 0.621s, episode steps: 145, steps per second: 233, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 48.650625, mae: 80.266596, mean_q: 163.151629, mean_eps: 0.307585\n",
      " 15654/20000: episode: 296, duration: 0.745s, episode steps: 194, steps per second: 260, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 57.244026, mae: 80.412419, mean_q: 163.093050, mean_eps: 0.299957\n",
      " 15854/20000: episode: 297, duration: 0.769s, episode steps: 200, steps per second: 260, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 40.075343, mae: 80.691056, mean_q: 163.821173, mean_eps: 0.291092\n",
      " 16005/20000: episode: 298, duration: 0.586s, episode steps: 151, steps per second: 258, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 55.062998, mae: 81.227521, mean_q: 164.728830, mean_eps: 0.283195\n",
      " 16186/20000: episode: 299, duration: 0.709s, episode steps: 181, steps per second: 255, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 33.133260, mae: 81.486956, mean_q: 165.734462, mean_eps: 0.275725\n",
      " 16351/20000: episode: 300, duration: 0.634s, episode steps: 165, steps per second: 260, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 43.644084, mae: 81.445530, mean_q: 165.570525, mean_eps: 0.267940\n",
      " 16551/20000: episode: 301, duration: 0.784s, episode steps: 200, steps per second: 255, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 51.831164, mae: 81.116207, mean_q: 164.772828, mean_eps: 0.259728\n",
      " 16751/20000: episode: 302, duration: 0.799s, episode steps: 200, steps per second: 250, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 46.487818, mae: 81.100882, mean_q: 164.441181, mean_eps: 0.250727\n",
      " 16935/20000: episode: 303, duration: 0.713s, episode steps: 184, steps per second: 258, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 44.434838, mae: 81.083259, mean_q: 164.644098, mean_eps: 0.242087\n",
      " 17102/20000: episode: 304, duration: 0.641s, episode steps: 167, steps per second: 261, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 38.903469, mae: 81.641129, mean_q: 165.651697, mean_eps: 0.234190\n",
      " 17279/20000: episode: 305, duration: 0.695s, episode steps: 177, steps per second: 255, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 53.567043, mae: 81.631079, mean_q: 165.780094, mean_eps: 0.226450\n",
      " 17479/20000: episode: 306, duration: 0.751s, episode steps: 200, steps per second: 266, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 46.511967, mae: 80.752381, mean_q: 164.140943, mean_eps: 0.217967\n",
      " 17665/20000: episode: 307, duration: 0.727s, episode steps: 186, steps per second: 256, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 49.145364, mae: 81.373984, mean_q: 165.491935, mean_eps: 0.209282\n",
      " 17865/20000: episode: 308, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 52.848888, mae: 81.758087, mean_q: 165.543348, mean_eps: 0.200597\n",
      " 18065/20000: episode: 309, duration: 0.751s, episode steps: 200, steps per second: 266, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 40.780021, mae: 82.486588, mean_q: 167.562580, mean_eps: 0.191597\n",
      " 18265/20000: episode: 310, duration: 0.795s, episode steps: 200, steps per second: 252, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 44.421804, mae: 82.255367, mean_q: 167.100393, mean_eps: 0.182597\n",
      " 18412/20000: episode: 311, duration: 0.587s, episode steps: 147, steps per second: 250, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 55.943830, mae: 82.320022, mean_q: 166.905233, mean_eps: 0.174790\n",
      " 18595/20000: episode: 312, duration: 0.730s, episode steps: 183, steps per second: 251, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 47.200169, mae: 82.342967, mean_q: 167.081238, mean_eps: 0.167365\n",
      " 18791/20000: episode: 313, duration: 0.737s, episode steps: 196, steps per second: 266, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 46.732250, mae: 82.068121, mean_q: 166.675803, mean_eps: 0.158837\n",
      " 18991/20000: episode: 314, duration: 0.797s, episode steps: 200, steps per second: 251, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 36.519536, mae: 82.695112, mean_q: 167.689730, mean_eps: 0.149927\n",
      " 19191/20000: episode: 315, duration: 0.800s, episode steps: 200, steps per second: 250, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 37.983820, mae: 83.591742, mean_q: 169.700555, mean_eps: 0.140927\n",
      " 19391/20000: episode: 316, duration: 0.810s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 44.633439, mae: 84.546197, mean_q: 171.553982, mean_eps: 0.131927\n",
      " 19591/20000: episode: 317, duration: 0.769s, episode steps: 200, steps per second: 260, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 37.849765, mae: 84.843072, mean_q: 172.024076, mean_eps: 0.122927\n",
      " 19775/20000: episode: 318, duration: 0.696s, episode steps: 184, steps per second: 264, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 54.143556, mae: 84.460978, mean_q: 170.932181, mean_eps: 0.114287\n",
      " 19975/20000: episode: 319, duration: 0.776s, episode steps: 200, steps per second: 258, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.019983, mae: 83.557109, mean_q: 169.389223, mean_eps: 0.105647\n",
      "done, took 78.342 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c314c31bb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "dqn.save_weights(f'my_weights_cartpole.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.load_weights(f'my_weights_cartpole.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f261d6473f1dd2b46c53affda8b45565a09c2039f31152146d1a5fcb65cff0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
