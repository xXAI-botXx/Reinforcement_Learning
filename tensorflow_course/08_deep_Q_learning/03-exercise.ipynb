{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advisory-score",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../COURSE_NOTEBOOKS/Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-novel",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-agency",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement your first keras-rl agent based on the **Acrobot** environment (https://gym.openai.com/envs/Acrobot-v1/) <br />\n",
    "The goal of this environment is to maneuver the robot arm upwards above the line with as little steps as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-conducting",
   "metadata": {},
   "source": [
    "**TASK: Import necessary libraries** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "knowing-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-tenant",
   "metadata": {},
   "source": [
    "**TASK: Create the environment** <br />\n",
    "The name is: *Acrobot-v1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compound-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Acrobot-v1\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "israeli-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 3\n",
      "Observation Space: (6,)\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape\n",
    "print(f\"Action Space: {env.action_space.n}\")\n",
    "print(f\"Observation Space: {num_observations}\")\n",
    "\n",
    "assert num_actions == 3 and num_observations == (6,) , \"Wrong environment!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-tuesday",
   "metadata": {},
   "source": [
    "**TASK: Create the Neural Network for your Deep-Q-Agent** <br />\n",
    "Take a look at the size of the action space and the size of the observation space.\n",
    "You are free to chose any architecture you want! <br />\n",
    "Hint: It already works with three layers, each having 64 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mexican-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=((1,)+num_observations)))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-mixture",
   "metadata": {},
   "source": [
    "**TASK: Initialize the circular buffer**<br />\n",
    "Make sure you set the limit appropriately (50000 works well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "short-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-grain",
   "metadata": {},
   "source": [
    "**TASK: Use the epsilon greedy action selection strategy with *decaying* epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "polished-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                                attr='eps',\n",
    "                                value_max=1.0,\n",
    "                                value_min=0.1,\n",
    "                                value_test=0.05,\n",
    "                                nb_steps=150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-straight",
   "metadata": {},
   "source": [
    "**TASK: Create the DQNAgent** <br />\n",
    "Feel free to play with the nb_steps_warump, target_model_update, batch_size and gamma parameters. <br />\n",
    "Hint:<br />\n",
    "You can try *nb_steps_warmup*=1000, *target_model_update*=1000, *batch_size*=32 and *gamma*=0.99 as a first guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "terminal-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=num_actions,\n",
    "                nb_steps_warmup=1000, target_model_update=1000, batch_size=32, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-shooting",
   "metadata": {},
   "source": [
    "**TASK: Compile the model** <br />\n",
    "Feel free to explore the effects of different optimizers and learning rates.\n",
    "You can try Adam with a learning rate of 1e-3 as a first guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "damaged-syracuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-belgium",
   "metadata": {},
   "source": [
    "**TASK: Fit the model** <br />\n",
    "150,000 steps should be a very good starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adverse-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 30000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500/30000: episode: 1, duration: 0.322s, episode steps: 500, steps per second: 1552, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1000/30000: episode: 2, duration: 0.260s, episode steps: 500, steps per second: 1922, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1500/30000: episode: 3, duration: 2.364s, episode steps: 500, steps per second: 212, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.008666, mae: 0.632068, mean_q: -0.886856, mean_eps: 0.943750\n",
      "  2000/30000: episode: 4, duration: 2.096s, episode steps: 500, steps per second: 239, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.000223, mae: 0.630398, mean_q: -0.918326, mean_eps: 0.921273\n",
      "  2500/30000: episode: 5, duration: 1.982s, episode steps: 500, steps per second: 252, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.007087, mae: 1.414952, mean_q: -2.069570, mean_eps: 0.898773\n",
      "  3000/30000: episode: 6, duration: 2.016s, episode steps: 500, steps per second: 248, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.001163, mae: 1.408325, mean_q: -2.078499, mean_eps: 0.876273\n",
      "  3500/30000: episode: 7, duration: 2.048s, episode steps: 500, steps per second: 244, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.011419, mae: 2.175903, mean_q: -3.209185, mean_eps: 0.853773\n",
      "  4000/30000: episode: 8, duration: 2.111s, episode steps: 500, steps per second: 237, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.004413, mae: 2.168775, mean_q: -3.211539, mean_eps: 0.831273\n",
      "  4500/30000: episode: 9, duration: 2.133s, episode steps: 500, steps per second: 234, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.014155, mae: 2.933469, mean_q: -4.341793, mean_eps: 0.808773\n",
      "  5000/30000: episode: 10, duration: 2.067s, episode steps: 500, steps per second: 242, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.013182, mae: 2.930097, mean_q: -4.339431, mean_eps: 0.786273\n",
      "  5500/30000: episode: 11, duration: 2.004s, episode steps: 500, steps per second: 249, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.026530, mae: 3.680875, mean_q: -5.439681, mean_eps: 0.763772\n",
      "  6000/30000: episode: 12, duration: 1.999s, episode steps: 500, steps per second: 250, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.022182, mae: 3.676237, mean_q: -5.443117, mean_eps: 0.741272\n",
      "  6500/30000: episode: 13, duration: 2.057s, episode steps: 500, steps per second: 243, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.035616, mae: 4.419044, mean_q: -6.534059, mean_eps: 0.718773\n",
      "  7000/30000: episode: 14, duration: 2.074s, episode steps: 500, steps per second: 241, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.026941, mae: 4.413977, mean_q: -6.539870, mean_eps: 0.696273\n",
      "  7500/30000: episode: 15, duration: 2.125s, episode steps: 500, steps per second: 235, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.816 [0.000, 2.000],  loss: 0.043997, mae: 5.154349, mean_q: -7.621735, mean_eps: 0.673772\n",
      "  8000/30000: episode: 16, duration: 2.134s, episode steps: 500, steps per second: 234, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.045728, mae: 5.148276, mean_q: -7.614894, mean_eps: 0.651273\n",
      "  8500/30000: episode: 17, duration: 2.120s, episode steps: 500, steps per second: 236, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.051508, mae: 5.902757, mean_q: -8.733619, mean_eps: 0.628773\n",
      "  9000/30000: episode: 18, duration: 2.123s, episode steps: 500, steps per second: 236, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 0.043944, mae: 5.896513, mean_q: -8.742452, mean_eps: 0.606272\n",
      "  9500/30000: episode: 19, duration: 2.109s, episode steps: 500, steps per second: 237, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 0.074595, mae: 6.609768, mean_q: -9.786438, mean_eps: 0.583772\n",
      " 10000/30000: episode: 20, duration: 2.299s, episode steps: 500, steps per second: 217, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.804 [0.000, 2.000],  loss: 0.067199, mae: 6.601767, mean_q: -9.789209, mean_eps: 0.561273\n",
      " 10500/30000: episode: 21, duration: 2.271s, episode steps: 500, steps per second: 220, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.848 [0.000, 2.000],  loss: 0.133922, mae: 7.308499, mean_q: -10.789638, mean_eps: 0.538772\n",
      " 11000/30000: episode: 22, duration: 2.309s, episode steps: 500, steps per second: 217, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.099003, mae: 7.302095, mean_q: -10.814261, mean_eps: 0.516273\n",
      " 11500/30000: episode: 23, duration: 2.142s, episode steps: 500, steps per second: 233, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.117624, mae: 7.965039, mean_q: -11.793963, mean_eps: 0.493772\n",
      " 11825/30000: episode: 24, duration: 1.496s, episode steps: 325, steps per second: 217, episode reward: -324.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.797 [0.000, 2.000],  loss: 0.177533, mae: 7.959954, mean_q: -11.748752, mean_eps: 0.475210\n",
      " 12172/30000: episode: 25, duration: 1.535s, episode steps: 347, steps per second: 226, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.100768, mae: 8.235645, mean_q: -12.188953, mean_eps: 0.460090\n",
      " 12672/30000: episode: 26, duration: 2.126s, episode steps: 500, steps per second: 235, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.158494, mae: 8.508562, mean_q: -12.596185, mean_eps: 0.441032\n",
      " 13097/30000: episode: 27, duration: 1.752s, episode steps: 425, steps per second: 243, episode reward: -424.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.150397, mae: 8.582409, mean_q: -12.684080, mean_eps: 0.420220\n",
      " 13597/30000: episode: 28, duration: 2.039s, episode steps: 500, steps per second: 245, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.148742, mae: 8.872558, mean_q: -13.122114, mean_eps: 0.399407\n",
      " 14097/30000: episode: 29, duration: 2.055s, episode steps: 500, steps per second: 243, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.886 [0.000, 2.000],  loss: 0.176719, mae: 8.968298, mean_q: -13.254724, mean_eps: 0.376907\n",
      " 14487/30000: episode: 30, duration: 1.551s, episode steps: 390, steps per second: 252, episode reward: -389.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.206531, mae: 9.430500, mean_q: -13.928629, mean_eps: 0.356882\n",
      " 14789/30000: episode: 31, duration: 1.226s, episode steps: 302, steps per second: 246, episode reward: -301.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.159432, mae: 9.409450, mean_q: -13.917310, mean_eps: 0.341312\n",
      " 15034/30000: episode: 32, duration: 0.976s, episode steps: 245, steps per second: 251, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.145746, mae: 9.482197, mean_q: -14.037364, mean_eps: 0.329005\n",
      " 15485/30000: episode: 33, duration: 1.792s, episode steps: 451, steps per second: 252, episode reward: -450.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.166946, mae: 9.936781, mean_q: -14.713095, mean_eps: 0.313345\n",
      " 15985/30000: episode: 34, duration: 1.971s, episode steps: 500, steps per second: 254, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.654 [0.000, 2.000],  loss: 0.157974, mae: 9.925450, mean_q: -14.703092, mean_eps: 0.291947\n",
      " 16374/30000: episode: 35, duration: 1.554s, episode steps: 389, steps per second: 250, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.756 [0.000, 2.000],  loss: 0.194924, mae: 10.394504, mean_q: -15.372663, mean_eps: 0.271945\n",
      " 16657/30000: episode: 36, duration: 1.112s, episode steps: 283, steps per second: 255, episode reward: -282.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.707 [0.000, 2.000],  loss: 0.108849, mae: 10.388079, mean_q: -15.397179, mean_eps: 0.256825\n",
      " 16797/30000: episode: 37, duration: 0.636s, episode steps: 140, steps per second: 220, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.686 [0.000, 2.000],  loss: 0.155369, mae: 10.384324, mean_q: -15.367007, mean_eps: 0.247307\n",
      " 17001/30000: episode: 38, duration: 0.869s, episode steps: 204, steps per second: 235, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.181026, mae: 10.360321, mean_q: -15.328534, mean_eps: 0.239567\n",
      " 17262/30000: episode: 39, duration: 1.090s, episode steps: 261, steps per second: 240, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.659 [0.000, 2.000],  loss: 0.339241, mae: 10.871485, mean_q: -16.024093, mean_eps: 0.229105\n",
      " 17572/30000: episode: 40, duration: 1.252s, episode steps: 310, steps per second: 248, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.232635, mae: 10.828326, mean_q: -15.996947, mean_eps: 0.216257\n",
      " 17961/30000: episode: 41, duration: 1.550s, episode steps: 389, steps per second: 251, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.591 [0.000, 2.000],  loss: 0.206893, mae: 10.803133, mean_q: -15.975138, mean_eps: 0.200530\n",
      " 18133/30000: episode: 42, duration: 0.682s, episode steps: 172, steps per second: 252, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.512 [0.000, 2.000],  loss: 0.333785, mae: 11.178960, mean_q: -16.484858, mean_eps: 0.187907\n",
      " 18517/30000: episode: 43, duration: 1.533s, episode steps: 384, steps per second: 251, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.612 [0.000, 2.000],  loss: 0.218378, mae: 11.296168, mean_q: -16.703097, mean_eps: 0.175397\n",
      " 18632/30000: episode: 44, duration: 0.508s, episode steps: 115, steps per second: 226, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.291653, mae: 11.239132, mean_q: -16.563184, mean_eps: 0.164170\n",
      " 18783/30000: episode: 45, duration: 0.669s, episode steps: 151, steps per second: 226, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.146107, mae: 11.301382, mean_q: -16.735658, mean_eps: 0.158185\n",
      " 18972/30000: episode: 46, duration: 0.859s, episode steps: 189, steps per second: 220, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.794 [0.000, 2.000],  loss: 0.167310, mae: 11.262833, mean_q: -16.661229, mean_eps: 0.150535\n",
      " 19282/30000: episode: 47, duration: 1.331s, episode steps: 310, steps per second: 233, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.748 [0.000, 2.000],  loss: 0.333124, mae: 11.664886, mean_q: -17.180505, mean_eps: 0.139307\n",
      " 19493/30000: episode: 48, duration: 0.958s, episode steps: 211, steps per second: 220, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.592 [0.000, 2.000],  loss: 0.224089, mae: 11.697859, mean_q: -17.289062, mean_eps: 0.127585\n",
      " 19702/30000: episode: 49, duration: 0.931s, episode steps: 209, steps per second: 224, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.284430, mae: 11.678717, mean_q: -17.244734, mean_eps: 0.118135\n",
      " 19943/30000: episode: 50, duration: 1.089s, episode steps: 241, steps per second: 221, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.264420, mae: 11.677500, mean_q: -17.239766, mean_eps: 0.108010\n",
      " 20179/30000: episode: 51, duration: 1.014s, episode steps: 236, steps per second: 233, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.187481, mae: 12.128470, mean_q: -17.948953, mean_eps: 0.100315\n",
      " 20587/30000: episode: 52, duration: 1.706s, episode steps: 408, steps per second: 239, episode reward: -407.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.684 [0.000, 2.000],  loss: 0.264679, mae: 12.217952, mean_q: -18.042389, mean_eps: 0.100000\n",
      " 20888/30000: episode: 53, duration: 1.314s, episode steps: 301, steps per second: 229, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.390137, mae: 12.205554, mean_q: -18.009616, mean_eps: 0.100000\n",
      " 21058/30000: episode: 54, duration: 0.866s, episode steps: 170, steps per second: 196, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.402099, mae: 12.305950, mean_q: -18.149455, mean_eps: 0.100000\n",
      " 21356/30000: episode: 55, duration: 1.363s, episode steps: 298, steps per second: 219, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.335344, mae: 12.504059, mean_q: -18.460238, mean_eps: 0.100000\n",
      " 21661/30000: episode: 56, duration: 1.326s, episode steps: 305, steps per second: 230, episode reward: -304.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.249 [0.000, 2.000],  loss: 0.247297, mae: 12.488483, mean_q: -18.453490, mean_eps: 0.100000\n",
      " 22079/30000: episode: 57, duration: 1.853s, episode steps: 418, steps per second: 226, episode reward: -417.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.368 [0.000, 2.000],  loss: 0.381484, mae: 12.491480, mean_q: -18.443198, mean_eps: 0.100000\n",
      " 22345/30000: episode: 58, duration: 1.147s, episode steps: 266, steps per second: 232, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.218 [0.000, 2.000],  loss: 0.306542, mae: 12.630634, mean_q: -18.647000, mean_eps: 0.100000\n",
      " 22558/30000: episode: 59, duration: 0.865s, episode steps: 213, steps per second: 246, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.300 [0.000, 2.000],  loss: 0.247740, mae: 12.596021, mean_q: -18.589878, mean_eps: 0.100000\n",
      " 22763/30000: episode: 60, duration: 0.866s, episode steps: 205, steps per second: 237, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.410 [0.000, 2.000],  loss: 0.202144, mae: 12.616711, mean_q: -18.647577, mean_eps: 0.100000\n",
      " 22938/30000: episode: 61, duration: 0.786s, episode steps: 175, steps per second: 223, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.310985, mae: 12.593641, mean_q: -18.568575, mean_eps: 0.100000\n",
      " 23179/30000: episode: 62, duration: 1.037s, episode steps: 241, steps per second: 232, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.498 [0.000, 2.000],  loss: 0.253195, mae: 12.763491, mean_q: -18.839972, mean_eps: 0.100000\n",
      " 23358/30000: episode: 63, duration: 0.786s, episode steps: 179, steps per second: 228, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.330605, mae: 12.788736, mean_q: -18.897487, mean_eps: 0.100000\n",
      " 23661/30000: episode: 64, duration: 1.233s, episode steps: 303, steps per second: 246, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.422 [0.000, 2.000],  loss: 0.305470, mae: 12.794547, mean_q: -18.895795, mean_eps: 0.100000\n",
      " 24161/30000: episode: 65, duration: 2.045s, episode steps: 500, steps per second: 244, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.622 [0.000, 2.000],  loss: 0.289887, mae: 12.913526, mean_q: -19.072926, mean_eps: 0.100000\n",
      " 24435/30000: episode: 66, duration: 1.140s, episode steps: 274, steps per second: 240, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.434 [0.000, 2.000],  loss: 0.221301, mae: 13.180876, mean_q: -19.489162, mean_eps: 0.100000\n",
      " 24692/30000: episode: 67, duration: 1.061s, episode steps: 257, steps per second: 242, episode reward: -256.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.290906, mae: 13.157719, mean_q: -19.425702, mean_eps: 0.100000\n",
      " 24904/30000: episode: 68, duration: 0.877s, episode steps: 212, steps per second: 242, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.259 [0.000, 2.000],  loss: 0.224103, mae: 13.154705, mean_q: -19.449666, mean_eps: 0.100000\n",
      " 25089/30000: episode: 69, duration: 0.742s, episode steps: 185, steps per second: 249, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.414817, mae: 13.401998, mean_q: -19.760410, mean_eps: 0.100000\n",
      " 25294/30000: episode: 70, duration: 0.822s, episode steps: 205, steps per second: 249, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.278 [0.000, 2.000],  loss: 0.386762, mae: 13.726280, mean_q: -20.247321, mean_eps: 0.100000\n",
      " 25448/30000: episode: 71, duration: 0.640s, episode steps: 154, steps per second: 240, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.481 [0.000, 2.000],  loss: 0.199669, mae: 13.640363, mean_q: -20.151421, mean_eps: 0.100000\n",
      " 25678/30000: episode: 72, duration: 0.928s, episode steps: 230, steps per second: 248, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.296 [0.000, 2.000],  loss: 0.278488, mae: 13.660404, mean_q: -20.182151, mean_eps: 0.100000\n",
      " 26013/30000: episode: 73, duration: 1.377s, episode steps: 335, steps per second: 243, episode reward: -334.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.354313, mae: 13.677235, mean_q: -20.190285, mean_eps: 0.100000\n",
      " 26402/30000: episode: 74, duration: 1.543s, episode steps: 389, steps per second: 252, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.224567, mae: 13.918469, mean_q: -20.585090, mean_eps: 0.100000\n",
      " 26902/30000: episode: 75, duration: 2.002s, episode steps: 500, steps per second: 250, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.283778, mae: 13.874629, mean_q: -20.515789, mean_eps: 0.100000\n",
      " 27122/30000: episode: 76, duration: 0.901s, episode steps: 220, steps per second: 244, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.396690, mae: 14.170299, mean_q: -20.943185, mean_eps: 0.100000\n",
      " 27545/30000: episode: 77, duration: 1.682s, episode steps: 423, steps per second: 251, episode reward: -422.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.555511, mae: 14.415629, mean_q: -21.257989, mean_eps: 0.100000\n",
      " 27919/30000: episode: 78, duration: 1.524s, episode steps: 374, steps per second: 245, episode reward: -373.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.250828, mae: 14.388083, mean_q: -21.285116, mean_eps: 0.100000\n",
      " 28323/30000: episode: 79, duration: 1.625s, episode steps: 404, steps per second: 249, episode reward: -403.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.305166, mae: 14.658237, mean_q: -21.669312, mean_eps: 0.100000\n",
      " 28653/30000: episode: 80, duration: 1.327s, episode steps: 330, steps per second: 249, episode reward: -329.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.385380, mae: 14.672541, mean_q: -21.641058, mean_eps: 0.100000\n",
      " 28888/30000: episode: 81, duration: 0.950s, episode steps: 235, steps per second: 247, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.505689, mae: 14.693480, mean_q: -21.666505, mean_eps: 0.100000\n",
      " 29203/30000: episode: 82, duration: 1.278s, episode steps: 315, steps per second: 246, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.756 [0.000, 2.000],  loss: 0.441690, mae: 15.014724, mean_q: -22.163958, mean_eps: 0.100000\n",
      " 29703/30000: episode: 83, duration: 1.993s, episode steps: 500, steps per second: 251, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.828 [0.000, 2.000],  loss: 0.364678, mae: 15.143485, mean_q: -22.381641, mean_eps: 0.100000\n",
      " 29927/30000: episode: 84, duration: 0.921s, episode steps: 224, steps per second: 243, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.367464, mae: 15.103907, mean_q: -22.321845, mean_eps: 0.100000\n",
      "done, took 122.233 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c9639aeb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, visualize=False, nb_steps=30000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c370969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIP] Next time specify overwrite=True!\n"
     ]
    }
   ],
   "source": [
    "dqn.save_weights('my_weights_acrobat', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-nightmare",
   "metadata": {},
   "source": [
    "**TASK: Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "framed-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py:51: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -500.000, steps: 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0faff8f10b40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[0;32m    350\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[1;34m(self, action, logs)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'on_action_end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\rl\\callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[1;34m(self, action, logs)\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 )\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[0;32m    432\u001b[0m         \u001b[1;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 )\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 )\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[0;32m    432\u001b[0m         \u001b[1;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 )\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\wrappers\\env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv_render_passive_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 )\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\envs\\classic_control\\acrobot.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_renders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tobia\\.conda\\envs\\ai\\lib\\site-packages\\gym\\envs\\classic_control\\acrobot.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"render_fps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Der Kernel ist beim Ausfhren von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestrzt. Bitte berprfen Sie den Code in der/den Zelle(n), um eine mgliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "dqn.test(env, visualize=True, nb_episodes=5)\n",
    "env.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f261d6473f1dd2b46c53affda8b45565a09c2039f31152146d1a5fcb65cff0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
